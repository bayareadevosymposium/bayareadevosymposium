---
layout: page
permalink: /schedule
title: Schedule
description: 
nav: true
nav_order: 3
pretty_table: true
---

This year's Bay Area Developmental Symposium will be held at Stanford University in Palo Alto, CA, on <b>Thursday, May 9, 2024</b>. The event will start around lunchtime (~12pm) and end after a group dinner (~6-8pm, open end).
<br><br>
The full schedule includes <b>a poster session, two talk sessions, as well as lunch and dinner</b> to meet and talk with the other attendees. For details, see the schedule below!

| Time | Duration | Event |
| :----------- | :----------- | :----------- |
| 12pm             |   -              |Arrive at Stanford, lunch is provided |
| 12:30-12:45pm    |   15 minutes     |Opening remarks|
| 12:45-1:30pm     |   45 minutes     |Faculty roundtable |
| 1:30-1:40pm      |   10 minutes     |Presenters: Set up talk equipment |
| 1:40-2:55pm      |   75 minutes     |Talk session 1|
| 2:55-3:10pm      |   15 minutes     |Coffee break|
| 3:10-4:10pm      |   60 minutes     |Poster session|
| 4:10-4:25pm      |   15 minutes     |Coffee break|
| 4:25-5:40pm      |   75 minutes     |Talk session 2|
| 5:40-5:50pm      |   10 minutes     |Stretch break|
| 5:50-6:00pm      |   10 minutes     |Closing remarks|
| 6 pm             |   -              |Rooftop dinner|

<br><br>

<h2>Talks</h2>
Talks will be 15 minutes each (including Q&A). There are two talk sessions, each including presentations centered around a broader theme:

<b>Talk Session 1: Language Development and Visual Attention</b>
<button class="accordion">The first known case of DeafBlind children learning conventionalized “Protactile” language. <em>Jenny Lu, UC Berkeley</em></button>
<div class="panel">
  <p>In this talk, I will discuss the first known case of DeafBlind children learning conventionalized “Protactile” language and interactive practices in the tactile modality. Protactile language is a new emerging tactile language, but to our knowledge has not yet been acquired by DeafBlind children. Bringing anthropological and psychological approaches together, we ask whether joint attention towards a shared point of reference is a product of socialization and cultural learning or is a skill that develops on a maturational timetable. Because these children have resided in sighted environments with hearing, sighted parents without much perceptible access to others’ actions and language input, it is an open question whether they readily develop tactile joint attention, which sets the foundation for learning Protactile phonology and deictic references (e.g., pointing). We present a case study of two DeafBlind children (n=2; 3;3-3;5 years) and analyze their naturalistic interactions with a DeafBlind adult who is a speaker of Protactile language. Using grounded ethnographic fieldnotes and linguistic analysis, we find that upon exposure to a DeafBlind language model, these children were gradually socialized to perform, perceive, and establish contact in the tactile modality in a way that is consistent with conventional articulatory mechanisms organized by protactile phonology and interactional practices, including joint attention. Secondly, the linguistic analysis demonstrates that the adult adjusts their use of language by innovating new constructions and functions in Protactile language. Both children and adults contribute to language emergence as they adjust to one another in the unfolding of interaction. Our findings suggest that language development and emergence are shaped, in part, by socially and culturally specific patterns of perception and attention.</p>
</div>

<button class="accordion">Syntactic Category Bias in Early Bilingual Vocabularies. <em>Alvin Tan, Stanford</em></button>
<div class="panel">
  <p>Previous research (e.g., Frank et al., 2021) has established that many languages exhibit a positive noun bias, such that young children know more nouns than would be expected by chance (relative to predicates and function words), although the magnitude of this bias varies across languages. Relatively fewer studies have investigated these biases in bilingual populations, whereby the interaction between the two languages being acquired may result in different bias patterns. In this work, we conduct reanalyses of Communicative Development Inventory data from bilinguals, showing that syntactic category bias is not affected by the other language being acquired, but is affected by the cultural context of the child.</p>
</div>

<button class="accordion">Estimating Age-Related Change in Infants’ Linguistic and Cognitive Development Using (Meta-)Meta-Analysis. <em>Anjie Cao, Stanford</em></button>
<div class="panel">
  <p>Developmental psychology focuses on how psychological phenomena emerge with age. In cognitive development research, however, the specifics of this emergence is often underspecified. Researchers often provisionally assume linear growth by including chronological age as a predictor in regression models. In this work, we aim to evaluate this assumption by examining the functional form of age trajectories across 25 phenomena in early linguistic and cognitive development using (meta-)meta-analysis. Surprisingly, for most meta-analyses, the effect size for the phenomenon was relatively constant throughout development. We investigated four possible hypotheses explaining this pattern: (1) age-related selection bias against younger infants; (2) methodological adaptation for older infants; (3) change in only a subset of conditions; and (4) positive growth only after infancy. None of these explained the lack of age-related growth in most datasets. Our work challenges the assumption of linear growth in early cognitive development and suggests the importance of uniform measurement across children of different ages.</p>
</div> 

<button class="accordion">From Pixels to Perception: Investigating Developing Visual Attention Through Convolutional Neural Networks. <em>Shannon Klotz, UC Davis</em></button>
<div class="panel">
  <p>For decades, studies have indicated that infants are more attentive to visually complex patterns, which feature multiple elements (Berlyne, 1958; Cohen et al., 1975; Horowitz et al., 1972; Hutt & McGrew, 1969). Defining visual complexity in natural scenes has often led to simplistic interpretations. We propose a novel quantitative model using AlexNet, a convolutional neural network inspired by the brain's object recognition pathway. This network processes images through layers that mimic increasing levels of complexity, quantifying scene complexity at various abstraction levels. Our study examined the gaze patterns of children from three age groups: 92 infants (4-12 months), 82 toddlers (12-48 months), and 47 preschoolers (12-36 months), who viewed digitized photographs of natural scenes. We correlated their gaze durations with activations in five specific AlexNet layers that represent different stages of visual processing, from basic shapes and colors to complex object classifications. Findings revealed that children’s gaze durations were generally longer for scenes triggering higher activations across all layers, suggesting a preference for more complex scenes. However, the pattern varied by age. Infants showed prolonged gaze only at higher activations in the early processing layers and not in the most abstract layer, FC3 (rs(2574) = 0.02, p = .23), indicating that the youngest infants focus more on physical characteristics than on abstract content. A developmental shift was noted; older infants preferred more complex images at the lower processing levels significantly more than younger infants (ß = .90, z = 8.81, p < .001). This aligns with the literature indicating that infants' attention evolves from being predominantly influenced by physical salience to being shaped by more complex, higher-order properties as they grow. Our results confirm that as children develop, visual complexity, defined through higher abstraction levels in neural processing, increasingly impacts their attention. This supports the use of models like AlexNet to quantitatively assess complexity in visual scenes and highlights significant developmental changes in visual preference.</p>
</div> 

<button class="accordion">Parent-Child Conversations about Science “Misconceptions.” <em>Sam McHugh, UCSC</em></button>
<div class="panel">
  <p>TBD!</p>
</div> 

<b>Talk Session 2: Socialization, Emotion, and Cultural Interactions</b>
<ul>
    <li>Interviews with AAPI Parents on Racial Socialization. <em>Victoria Keating, UC Berkeley</em></li>
    <li>Infant-Centered Behavioral Response Patterns to Discrete Emotions. <em>Zeynep Ozden, UC Merced</em></li>
    <li>Emotion Brokering: Helping Others Navigate Intercultural Emotion-Based Misunderstandings. <em>Sivenesi Subramoney, UC Merced</em></li>
    <li>Does Current Ecological Relevance Attenuate the Effects of Chaotic Home Environments on Children’s Inhibitory Control Performance? <em>Diego Placido, UC Davis</em></li>
    <li>Young Children’s and Caregivers’ Evaluations about Household Helping in the U.S. <em>Margie Martinez, UCSC</em></li>
</ul>

<h2>Posters</h2>
There will be one poster session from 3:10-4:10pm, <b>held outdoors</b> in the Philosophy Courtyard. We recognize that having one poster session might not allow you to easily see others’ posters; however, we will have the poster boards available starting at 12pm when you arrive, so you are welcome to hang up your poster then and look at the other posters during the coffee breaks (see the full schedule for the symposium here). We also encourage you to walk around during the poster session, as it is not mandatory that you stay at your poster the entire time.
 
The <b>maximum size for your poster is 4 feet (height) x 8 feet (width)</b> but you do not need to use the full space. Push pins will be provided!

<script>
    var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");

    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>